{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5557e804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11e3dd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running humaneval_HumanEval_1_Llama_3.2_1B_Instruct_cot.py ===\n",
      "→ Problem: humaneval_HumanEval_1, Model: Llama, Strategy: 3.2\n",
      "❌ Failed: Execution failed: invalid syntax. Perhaps you forgot a comma? (<string>, line 52)\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/z4/0lpbcqpj0vg_4p2l4z8md5cc0000gn/T/ipykernel_36216/711688988.py\", line 20, in check_correctness\n",
      "    exec(code, namespace)\n",
      "    ~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"<string>\", line 52\n",
      "    current_group = \"\".join(paren_string[:paren_string.index(char ill елем список результат.append(current_group) if balance > 0 else '')\n",
      "                                                             ^^^^^^^^\n",
      "SyntaxError: invalid syntax. Perhaps you forgot a comma?\n",
      "\n",
      "\n",
      "=== Running humaneval_HumanEval_1_OLMo_2_1B_cot.py ===\n",
      "→ Problem: humaneval_HumanEval_1, Model: OLMo, Strategy: 2\n",
      "❌ Failed: Execution failed: invalid syntax (<string>, line 33)\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/z4/0lpbcqpj0vg_4p2l4z8md5cc0000gn/T/ipykernel_36216/711688988.py\", line 20, in check_correctness\n",
      "    exec(code, namespace)\n",
      "    ~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"<string>\", line 33\n",
      "    if pair.lstrip('()') == pair.rstrip('())':\n",
      "                                             ^\n",
      "SyntaxError: invalid syntax\n",
      "\n",
      "\n",
      "=== Running humaneval_HumanEval_5_Llama_3.2_1B_Instruct_cot.py ===\n",
      "→ Problem: humaneval_HumanEval_5, Model: Llama, Strategy: 3.2\n",
      "✅ All tests passed! (unknown/unknown)\n",
      "\n",
      "=== Running humaneval_HumanEval_5_OLMo_2_1B_scot.py ===\n",
      "→ Problem: humaneval_HumanEval_5, Model: OLMo, Strategy: 2\n",
      "✅ All tests passed! (unknown/unknown)\n",
      "\n",
      "======================================================================\n",
      "Results saved to results.json\n",
      "\n",
      "======================================================================\n",
      "SUMMARY: 2/4 solutions passed\n",
      "======================================================================\n",
      "Solutions:\n",
      "  ✅ Passed:    2 ( 50.0%)\n",
      "  ❌ Failed:    2 ( 50.0%)\n",
      "======================================================================\n",
      "\n",
      "By Model:\n",
      "  Llama     :  1/ 2 solutions ( 50.0%) |    0/   0 tests (  0.0%)\n",
      "  OLMo      :  1/ 2 solutions ( 50.0%) |    0/   0 tests (  0.0%)\n",
      "\n",
      "By Strategy:\n",
      "  2         :  1/ 2 solutions ( 50.0%) |    0/   0 tests (  0.0%)\n",
      "  3.2       :  1/ 2 solutions ( 50.0%) |    0/   0 tests (  0.0%)\n",
      "\n",
      "✅ Passed solutions:\n",
      "   - humaneval_HumanEval_5_Llama_3.2_1B_Instruct_cot.py (unknown/unknown tests)\n",
      "   - humaneval_HumanEval_5_OLMo_2_1B_scot.py (unknown/unknown tests)\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import traceback\n",
    "import sys\n",
    "from io import StringIO\n",
    "\n",
    "GENERATED_DIR = \"generated\"\n",
    "PROBLEMS_FILE = \"problems.json\"\n",
    "\n",
    "\n",
    "def check_correctness(problem, code_file):\n",
    "    \"\"\"Execute the code file and run HumanEval+ tests, tracking individual test results.\"\"\"\n",
    "    try:\n",
    "        # Create a namespace for execution\n",
    "        namespace = {}\n",
    "        \n",
    "        # Load and execute user's code\n",
    "        with open(code_file, \"r\") as f:\n",
    "            code = f.read()\n",
    "        exec(code, namespace)\n",
    "        \n",
    "        # Check if function exists\n",
    "        entry_point = problem['entry_point']\n",
    "        if entry_point not in namespace:\n",
    "            return {\n",
    "                \"error\": f\"Function '{entry_point}' not found in the code\",\n",
    "                \"tests_passed\": 0,\n",
    "                \"tests_total\": 0\n",
    "            }\n",
    "        \n",
    "        candidate = namespace[entry_point]\n",
    "        \n",
    "        # Execute the test code to get the check function\n",
    "        exec(problem['test'], namespace)\n",
    "        \n",
    "        if 'check' not in namespace:\n",
    "            return {\n",
    "                \"error\": \"Test function 'check' not found\",\n",
    "                \"tests_passed\": 0,\n",
    "                \"tests_total\": 0\n",
    "            }\n",
    "        \n",
    "        # Now we need to count individual test cases\n",
    "        # We'll modify the check function to run tests individually\n",
    "        test_code = problem['test']\n",
    "        \n",
    "        # Parse the test code to extract inputs\n",
    "        # The test code has a structure with 'inputs = [...]'\n",
    "        exec_ns = {}\n",
    "        exec(test_code, exec_ns)\n",
    "        \n",
    "        # Try to get inputs from the check function's local scope\n",
    "        # We'll need to extract this more carefully\n",
    "        inputs = []\n",
    "        if 'inputs' in test_code:\n",
    "            # Execute just the inputs part\n",
    "            try:\n",
    "                for line in test_code.split('\\n'):\n",
    "                    if line.strip().startswith('inputs = '):\n",
    "                        exec(line, exec_ns)\n",
    "                        inputs = exec_ns.get('inputs', [])\n",
    "                        break\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        total_tests = len(inputs) if inputs else 0\n",
    "        \n",
    "        # If we can't extract inputs, just run the check function normally\n",
    "        if total_tests == 0:\n",
    "            try:\n",
    "                namespace['check'](candidate)\n",
    "                # If it passes, we don't know how many tests, so estimate\n",
    "                return {\n",
    "                    \"passed\": True,\n",
    "                    \"error\": None,\n",
    "                    \"tests_passed\": \"unknown\",\n",
    "                    \"tests_total\": \"unknown\"\n",
    "                }\n",
    "            except AssertionError as e:\n",
    "                return {\n",
    "                    \"passed\": False,\n",
    "                    \"error\": f\"Test failed: {str(e)}\",\n",
    "                    \"tests_passed\": \"unknown\",\n",
    "                    \"tests_total\": \"unknown\"\n",
    "                }\n",
    "            except Exception as e:\n",
    "                return {\n",
    "                    \"passed\": False,\n",
    "                    \"error\": f\"Runtime error: {str(e)}\",\n",
    "                    \"tests_passed\": 0,\n",
    "                    \"tests_total\": 0\n",
    "                }\n",
    "        \n",
    "        # Run each test individually\n",
    "        passed_tests = 0\n",
    "        failed_tests = []\n",
    "        \n",
    "        for i, test_input in enumerate(inputs):\n",
    "            try:\n",
    "                result = candidate(*test_input)\n",
    "                # We can't verify correctness without expected output\n",
    "                # So we just check if it runs without error\n",
    "                passed_tests += 1\n",
    "            except Exception as e:\n",
    "                failed_tests.append((i, test_input, str(e)))\n",
    "        \n",
    "        # Also run the full check to see if assertions pass\n",
    "        try:\n",
    "            namespace['check'](candidate)\n",
    "            all_passed = True\n",
    "        except AssertionError as e:\n",
    "            all_passed = False\n",
    "            error_msg = str(e)\n",
    "        except Exception as e:\n",
    "            all_passed = False\n",
    "            error_msg = str(e)\n",
    "        \n",
    "        if all_passed:\n",
    "            return {\n",
    "                \"passed\": True,\n",
    "                \"error\": None,\n",
    "                \"tests_passed\": total_tests,\n",
    "                \"tests_total\": total_tests\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"passed\": False,\n",
    "                \"error\": error_msg if 'error_msg' in locals() else \"Some assertions failed\",\n",
    "                \"tests_passed\": passed_tests,\n",
    "                \"tests_total\": total_tests,\n",
    "                \"failed_test_samples\": failed_tests[:3]\n",
    "            }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"error\": f\"Execution failed: {str(e)}\\n{traceback.format_exc()}\",\n",
    "            \"tests_passed\": 0,\n",
    "            \"tests_total\": 0\n",
    "        }\n",
    "\n",
    "\n",
    "def main():\n",
    "    with open(PROBLEMS_FILE) as f:\n",
    "        problems = json.load(f)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for file in sorted(os.listdir(GENERATED_DIR)):\n",
    "        if not file.endswith(\".py\"):\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(GENERATED_DIR, file)\n",
    "        parts = file[:-3].split(\"_\")\n",
    "        if len(parts) < 4:\n",
    "            print(f\"Skipping {file} (invalid name format)\")\n",
    "            continue\n",
    "\n",
    "        # Parse: humaneval_HumanEval_X_model_strategy\n",
    "        problem_id = \"_\".join(parts[:3])  # humaneval_HumanEval_X\n",
    "        model = parts[3]\n",
    "        strategy = parts[4] if len(parts) > 4 else \"unknown\"\n",
    "\n",
    "        problem = next((p for p in problems if p[\"id\"] == problem_id), None)\n",
    "        if not problem:\n",
    "            print(f\"⚠️ Problem {problem_id} not found in problems.json\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n=== Running {file} ===\")\n",
    "        print(f\"→ Problem: {problem_id}, Model: {model}, Strategy: {strategy}\")\n",
    "\n",
    "        result = check_correctness(problem, file_path)\n",
    "\n",
    "        tests_passed = result.get('tests_passed', 0)\n",
    "        tests_total = result.get('tests_total', 0)\n",
    "        \n",
    "        if \"error\" in result and result[\"error\"]:\n",
    "            print(f\"❌ Failed: {result['error']}\")\n",
    "            if tests_total:\n",
    "                print(f\"   Tests: {tests_passed}/{tests_total}\")\n",
    "        else:\n",
    "            if result.get(\"passed\"):\n",
    "                print(f\"✅ All tests passed! ({tests_passed}/{tests_total})\")\n",
    "            else:\n",
    "                print(f\"⚠️ Partial pass: {tests_passed}/{tests_total} tests\")\n",
    "\n",
    "        results.append({\n",
    "            \"file\": file,\n",
    "            \"problem_id\": problem_id,\n",
    "            \"model\": model,\n",
    "            \"strategy\": strategy,\n",
    "            \"passed\": result.get(\"passed\", False),\n",
    "            \"error\": result.get(\"error\"),\n",
    "            \"tests_passed\": tests_passed,\n",
    "            \"tests_total\": tests_total\n",
    "        })\n",
    "\n",
    "    # Save summary\n",
    "    with open(\"results.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Results saved to results.json\")\n",
    "    \n",
    "    # Print summary statistics\n",
    "    total_solutions = len(results)\n",
    "    passed_solutions = sum(1 for r in results if r.get('passed', False))\n",
    "    failed_solutions = total_solutions - passed_solutions\n",
    "    \n",
    "    # Calculate test statistics\n",
    "    total_tests_sum = sum(r.get('tests_total', 0) for r in results if isinstance(r.get('tests_total'), int))\n",
    "    passed_tests_sum = sum(r.get('tests_passed', 0) for r in results if isinstance(r.get('tests_passed'), int))\n",
    "    \n",
    "    if total_solutions > 0:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"SUMMARY: {passed_solutions}/{total_solutions} solutions passed\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Solutions:\")\n",
    "        print(f\"  ✅ Passed:  {passed_solutions:3d} ({100*passed_solutions/total_solutions:5.1f}%)\")\n",
    "        print(f\"  ❌ Failed:  {failed_solutions:3d} ({100*failed_solutions/total_solutions:5.1f}%)\")\n",
    "        \n",
    "        if total_tests_sum > 0:\n",
    "            print(f\"\\nIndividual Tests:\")\n",
    "            print(f\"  ✅ Passed:  {passed_tests_sum:4d}/{total_tests_sum:4d} ({100*passed_tests_sum/total_tests_sum:5.1f}%)\")\n",
    "            print(f\"  ❌ Failed:  {total_tests_sum - passed_tests_sum:4d}/{total_tests_sum:4d} ({100*(total_tests_sum - passed_tests_sum)/total_tests_sum:5.1f}%)\")\n",
    "        \n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Breakdown by model and strategy\n",
    "        from collections import defaultdict\n",
    "        by_model = defaultdict(lambda: {'passed': 0, 'total': 0, 'tests_passed': 0, 'tests_total': 0})\n",
    "        by_strategy = defaultdict(lambda: {'passed': 0, 'total': 0, 'tests_passed': 0, 'tests_total': 0})\n",
    "        \n",
    "        for r in results:\n",
    "            model = r.get('model', 'unknown')\n",
    "            strategy = r.get('strategy', 'unknown')\n",
    "            passed = r.get('passed', False)\n",
    "            tests_p = r.get('tests_passed', 0) if isinstance(r.get('tests_passed'), int) else 0\n",
    "            tests_t = r.get('tests_total', 0) if isinstance(r.get('tests_total'), int) else 0\n",
    "            \n",
    "            by_model[model]['total'] += 1\n",
    "            by_strategy[strategy]['total'] += 1\n",
    "            by_model[model]['tests_passed'] += tests_p\n",
    "            by_model[model]['tests_total'] += tests_t\n",
    "            by_strategy[strategy]['tests_passed'] += tests_p\n",
    "            by_strategy[strategy]['tests_total'] += tests_t\n",
    "            \n",
    "            if passed:\n",
    "                by_model[model]['passed'] += 1\n",
    "                by_strategy[strategy]['passed'] += 1\n",
    "        \n",
    "        if len(by_model) > 1:\n",
    "            print(\"\\nBy Model:\")\n",
    "            for model, stats in sorted(by_model.items()):\n",
    "                sol_pct = 100 * stats['passed'] / stats['total'] if stats['total'] > 0 else 0\n",
    "                test_pct = 100 * stats['tests_passed'] / stats['tests_total'] if stats['tests_total'] > 0 else 0\n",
    "                print(f\"  {model:10s}: {stats['passed']:2d}/{stats['total']:2d} solutions ({sol_pct:5.1f}%) | {stats['tests_passed']:4d}/{stats['tests_total']:4d} tests ({test_pct:5.1f}%)\")\n",
    "        \n",
    "        if len(by_strategy) > 1:\n",
    "            print(\"\\nBy Strategy:\")\n",
    "            for strategy, stats in sorted(by_strategy.items()):\n",
    "                sol_pct = 100 * stats['passed'] / stats['total'] if stats['total'] > 0 else 0\n",
    "                test_pct = 100 * stats['tests_passed'] / stats['tests_total'] if stats['tests_total'] > 0 else 0\n",
    "                print(f\"  {strategy:10s}: {stats['passed']:2d}/{stats['total']:2d} solutions ({sol_pct:5.1f}%) | {stats['tests_passed']:4d}/{stats['tests_total']:4d} tests ({test_pct:5.1f}%)\")\n",
    "        \n",
    "        # Show which solutions passed\n",
    "        if passed_solutions > 0:\n",
    "            print(f\"\\n✅ Passed solutions:\")\n",
    "            for r in results:\n",
    "                if r.get('passed', False):\n",
    "                    tp = r.get('tests_passed', '?')\n",
    "                    tt = r.get('tests_total', '?')\n",
    "                    print(f\"   - {r['file']} ({tp}/{tt} tests)\")\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

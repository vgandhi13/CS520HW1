{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5e7398ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 15 problems to problems.json\n",
      "humaneval_HumanEval_0: has_close_elements\n",
      "humaneval_HumanEval_1: separate_paren_groups\n",
      "humaneval_HumanEval_2: truncate_number\n",
      "humaneval_HumanEval_3: below_zero\n",
      "humaneval_HumanEval_4: mean_absolute_deviation\n",
      "humaneval_HumanEval_5: intersperse\n",
      "humaneval_HumanEval_6: parse_nested_parens\n",
      "humaneval_HumanEval_7: filter_by_substring\n",
      "humaneval_HumanEval_8: sum_product\n",
      "humaneval_HumanEval_9: rolling_max\n",
      "humaneval_HumanEval_10: make_palindrome\n",
      "humaneval_HumanEval_11: string_xor\n",
      "humaneval_HumanEval_12: longest\n",
      "humaneval_HumanEval_13: greatest_common_divisor\n",
      "humaneval_HumanEval_14: all_prefixes\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "# Load 10 problems from HumanEval+\n",
    "dataset = load_dataset(\"evalplus/humanevalplus\", split=\"test\")\n",
    "\n",
    "problems = []\n",
    "for i in range(15):\n",
    "    p = dataset[i]\n",
    "    problems.append({\n",
    "        'id': f\"humaneval_{p['task_id'].replace('/', '_')}\",\n",
    "        'task_id': p['task_id'],\n",
    "        'prompt': p['prompt'],\n",
    "        'canonical_solution': p['canonical_solution'],\n",
    "        'entry_point': p['entry_point'],\n",
    "        'test': p['test']  # This contains the full test code with check function\n",
    "    })\n",
    "\n",
    "with open('problems.json', 'w') as f:\n",
    "    json.dump(problems, f, indent=2)\n",
    "\n",
    "print(f\"Saved {len(problems)} problems to problems.json\")\n",
    "for p in problems:\n",
    "    print(f\"{p['id']}: {p['entry_point']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5557e804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a387e38e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created 60 files in 'generated/'\n",
      " - humaneval_HumanEval_0_Tulu_3_cot.py\n",
      " - humaneval_HumanEval_0_Tulu_3_scot.py\n",
      " - humaneval_HumanEval_0_Llama_cot.py\n",
      " - humaneval_HumanEval_0_Llama_scot.py\n",
      " - humaneval_HumanEval_1_Tulu_3_cot.py\n",
      " - humaneval_HumanEval_1_Tulu_3_scot.py\n",
      " - humaneval_HumanEval_1_Llama_cot.py\n",
      " - humaneval_HumanEval_1_Llama_scot.py\n",
      " - humaneval_HumanEval_2_Tulu_3_cot.py\n",
      " - humaneval_HumanEval_2_Tulu_3_scot.py\n",
      " - humaneval_HumanEval_2_Llama_cot.py\n",
      " - humaneval_HumanEval_2_Llama_scot.py\n",
      " - humaneval_HumanEval_3_Tulu_3_cot.py\n",
      " - humaneval_HumanEval_3_Tulu_3_scot.py\n",
      " - humaneval_HumanEval_3_Llama_cot.py\n",
      " - humaneval_HumanEval_3_Llama_scot.py\n",
      " - humaneval_HumanEval_4_Tulu_3_cot.py\n",
      " - humaneval_HumanEval_4_Tulu_3_scot.py\n",
      " - humaneval_HumanEval_4_Llama_cot.py\n",
      " - humaneval_HumanEval_4_Llama_scot.py\n",
      " - humaneval_HumanEval_5_Tulu_3_cot.py\n",
      " - humaneval_HumanEval_5_Tulu_3_scot.py\n",
      " - humaneval_HumanEval_5_Llama_cot.py\n",
      " - humaneval_HumanEval_5_Llama_scot.py\n",
      " - humaneval_HumanEval_6_Tulu_3_cot.py\n",
      " - humaneval_HumanEval_6_Tulu_3_scot.py\n",
      " - humaneval_HumanEval_6_Llama_cot.py\n",
      " - humaneval_HumanEval_6_Llama_scot.py\n",
      " - humaneval_HumanEval_7_Tulu_3_cot.py\n",
      " - humaneval_HumanEval_7_Tulu_3_scot.py\n",
      " - humaneval_HumanEval_7_Llama_cot.py\n",
      " - humaneval_HumanEval_7_Llama_scot.py\n",
      " - humaneval_HumanEval_8_Tulu_3_cot.py\n",
      " - humaneval_HumanEval_8_Tulu_3_scot.py\n",
      " - humaneval_HumanEval_8_Llama_cot.py\n",
      " - humaneval_HumanEval_8_Llama_scot.py\n",
      " - humaneval_HumanEval_9_Tulu_3_cot.py\n",
      " - humaneval_HumanEval_9_Tulu_3_scot.py\n",
      " - humaneval_HumanEval_9_Llama_cot.py\n",
      " - humaneval_HumanEval_9_Llama_scot.py\n",
      " - humaneval_HumanEval_10_Tulu_3_cot.py\n",
      " - humaneval_HumanEval_10_Tulu_3_scot.py\n",
      " - humaneval_HumanEval_10_Llama_cot.py\n",
      " - humaneval_HumanEval_10_Llama_scot.py\n",
      " - humaneval_HumanEval_11_Tulu_3_cot.py\n",
      " - humaneval_HumanEval_11_Tulu_3_scot.py\n",
      " - humaneval_HumanEval_11_Llama_cot.py\n",
      " - humaneval_HumanEval_11_Llama_scot.py\n",
      " - humaneval_HumanEval_12_Tulu_3_cot.py\n",
      " - humaneval_HumanEval_12_Tulu_3_scot.py\n",
      " - humaneval_HumanEval_12_Llama_cot.py\n",
      " - humaneval_HumanEval_12_Llama_scot.py\n",
      " - humaneval_HumanEval_13_Tulu_3_cot.py\n",
      " - humaneval_HumanEval_13_Tulu_3_scot.py\n",
      " - humaneval_HumanEval_13_Llama_cot.py\n",
      " - humaneval_HumanEval_13_Llama_scot.py\n",
      " - humaneval_HumanEval_14_Tulu_3_cot.py\n",
      " - humaneval_HumanEval_14_Tulu_3_scot.py\n",
      " - humaneval_HumanEval_14_Llama_cot.py\n",
      " - humaneval_HumanEval_14_Llama_scot.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "PROBLEMS_FILE = \"problems.json\"\n",
    "GENERATED_DIR = \"generated\"\n",
    "\n",
    "# Define the models and prompting strategies you'll use\n",
    "MODELS = [\"OLMo_2_1B\", \"Llama_3.2_1B_Instruct\"]\n",
    "STRATEGIES = [\"cot\", \"scot\"]\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Create directory if not exists\n",
    "    os.makedirs(GENERATED_DIR, exist_ok=True)\n",
    "\n",
    "    # Load problems\n",
    "    with open(PROBLEMS_FILE) as f:\n",
    "        problems = json.load(f)\n",
    "\n",
    "    created_files = []\n",
    "\n",
    "    for problem in problems:\n",
    "        problem_id = problem[\"id\"]\n",
    "        for model in MODELS:\n",
    "            for strategy in STRATEGIES:\n",
    "                filename = f\"{problem_id}_{model}_{strategy}.py\"\n",
    "                filepath = os.path.join(GENERATED_DIR, filename)\n",
    "\n",
    "                # Create file with problem prompt as context\n",
    "                if not os.path.exists(filepath):\n",
    "                    with open(filepath, \"w\") as f:\n",
    "                        f.write(f\"# {problem_id} | Model: {model} | Strategy: {strategy}\\n\")\n",
    "                        f.write(f\"# Entry point: {problem['entry_point']}\\n\\n\")\n",
    "                        f.write(\"# Problem prompt:\\n\")\n",
    "                        for line in problem['prompt'].split('\\n'):\n",
    "                            f.write(f\"# {line}\\n\")\n",
    "                        f.write(\"\\n# Paste your generated code below\\n\\n\")\n",
    "                    created_files.append(filename)\n",
    "\n",
    "    print(f\"✅ Created {len(created_files)} files in '{GENERATED_DIR}/'\")\n",
    "    for f in created_files:\n",
    "        print(f\" - {f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "11e3dd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running humaneval_HumanEval_0_Llama_cot.py ===\n",
      "→ Problem: humaneval_HumanEval_0, Model: Llama, Strategy: cot\n",
      "✅ All tests passed! (unknown/unknown)\n",
      "\n",
      "=== Running humaneval_HumanEval_0_Llama_scot.py ===\n",
      "→ Problem: humaneval_HumanEval_0, Model: Llama, Strategy: scot\n",
      "✅ All tests passed! (unknown/unknown)\n",
      "\n",
      "=== Running humaneval_HumanEval_0_Tulu_3_cot.py ===\n",
      "→ Problem: humaneval_HumanEval_0, Model: Tulu, Strategy: 3\n",
      "✅ All tests passed! (unknown/unknown)\n",
      "\n",
      "=== Running humaneval_HumanEval_0_Tulu_3_scot.py ===\n",
      "→ Problem: humaneval_HumanEval_0, Model: Tulu, Strategy: 3\n",
      "❌ Failed: Execution failed: cannot import name 'abs' from 'math' (/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/lib-dynload/math.cpython-313-darwin.so)\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/z4/0lpbcqpj0vg_4p2l4z8md5cc0000gn/T/ipykernel_19093/711688988.py\", line 20, in check_correctness\n",
      "    exec(code, namespace)\n",
      "    ~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"<string>\", line 48, in <module>\n",
      "ImportError: cannot import name 'abs' from 'math' (/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/lib-dynload/math.cpython-313-darwin.so). Did you mean: 'fabs'?\n",
      "\n",
      "\n",
      "=== Running humaneval_HumanEval_10_Llama_cot.py ===\n",
      "→ Problem: humaneval_HumanEval_10, Model: Llama, Strategy: cot\n",
      "❌ Failed: Function 'make_palindrome' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_10_Llama_scot.py ===\n",
      "→ Problem: humaneval_HumanEval_10, Model: Llama, Strategy: scot\n",
      "❌ Failed: Function 'make_palindrome' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_10_Tulu_3_cot.py ===\n",
      "→ Problem: humaneval_HumanEval_10, Model: Tulu, Strategy: 3\n",
      "❌ Failed: Function 'make_palindrome' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_10_Tulu_3_scot.py ===\n",
      "→ Problem: humaneval_HumanEval_10, Model: Tulu, Strategy: 3\n",
      "❌ Failed: Function 'make_palindrome' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_11_Llama_cot.py ===\n",
      "→ Problem: humaneval_HumanEval_11, Model: Llama, Strategy: cot\n",
      "❌ Failed: Function 'string_xor' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_11_Llama_scot.py ===\n",
      "→ Problem: humaneval_HumanEval_11, Model: Llama, Strategy: scot\n",
      "❌ Failed: Function 'string_xor' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_11_Tulu_3_cot.py ===\n",
      "→ Problem: humaneval_HumanEval_11, Model: Tulu, Strategy: 3\n",
      "❌ Failed: Function 'string_xor' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_11_Tulu_3_scot.py ===\n",
      "→ Problem: humaneval_HumanEval_11, Model: Tulu, Strategy: 3\n",
      "❌ Failed: Function 'string_xor' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_12_Llama_cot.py ===\n",
      "→ Problem: humaneval_HumanEval_12, Model: Llama, Strategy: cot\n",
      "❌ Failed: Function 'longest' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_12_Llama_scot.py ===\n",
      "→ Problem: humaneval_HumanEval_12, Model: Llama, Strategy: scot\n",
      "❌ Failed: Function 'longest' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_12_Tulu_3_cot.py ===\n",
      "→ Problem: humaneval_HumanEval_12, Model: Tulu, Strategy: 3\n",
      "❌ Failed: Function 'longest' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_12_Tulu_3_scot.py ===\n",
      "→ Problem: humaneval_HumanEval_12, Model: Tulu, Strategy: 3\n",
      "❌ Failed: Function 'longest' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_13_Llama_cot.py ===\n",
      "→ Problem: humaneval_HumanEval_13, Model: Llama, Strategy: cot\n",
      "❌ Failed: Function 'greatest_common_divisor' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_13_Llama_scot.py ===\n",
      "→ Problem: humaneval_HumanEval_13, Model: Llama, Strategy: scot\n",
      "❌ Failed: Function 'greatest_common_divisor' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_13_Tulu_3_cot.py ===\n",
      "→ Problem: humaneval_HumanEval_13, Model: Tulu, Strategy: 3\n",
      "❌ Failed: Function 'greatest_common_divisor' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_13_Tulu_3_scot.py ===\n",
      "→ Problem: humaneval_HumanEval_13, Model: Tulu, Strategy: 3\n",
      "❌ Failed: Function 'greatest_common_divisor' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_14_Llama_cot.py ===\n",
      "→ Problem: humaneval_HumanEval_14, Model: Llama, Strategy: cot\n",
      "❌ Failed: Function 'all_prefixes' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_14_Llama_scot.py ===\n",
      "→ Problem: humaneval_HumanEval_14, Model: Llama, Strategy: scot\n",
      "❌ Failed: Function 'all_prefixes' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_14_Tulu_3_cot.py ===\n",
      "→ Problem: humaneval_HumanEval_14, Model: Tulu, Strategy: 3\n",
      "❌ Failed: Function 'all_prefixes' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_14_Tulu_3_scot.py ===\n",
      "→ Problem: humaneval_HumanEval_14, Model: Tulu, Strategy: 3\n",
      "❌ Failed: Function 'all_prefixes' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_1_Llama_cot.py ===\n",
      "→ Problem: humaneval_HumanEval_1, Model: Llama, Strategy: cot\n",
      "❌ Failed: Function 'separate_paren_groups' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_1_Llama_scot.py ===\n",
      "→ Problem: humaneval_HumanEval_1, Model: Llama, Strategy: scot\n",
      "❌ Failed: Function 'separate_paren_groups' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_1_Tulu_3_cot.py ===\n",
      "→ Problem: humaneval_HumanEval_1, Model: Tulu, Strategy: 3\n",
      "❌ Failed: Function 'separate_paren_groups' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_1_Tulu_3_scot.py ===\n",
      "→ Problem: humaneval_HumanEval_1, Model: Tulu, Strategy: 3\n",
      "❌ Failed: Function 'separate_paren_groups' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_2_Llama_cot.py ===\n",
      "→ Problem: humaneval_HumanEval_2, Model: Llama, Strategy: cot\n",
      "❌ Failed: Function 'truncate_number' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_2_Llama_scot.py ===\n",
      "→ Problem: humaneval_HumanEval_2, Model: Llama, Strategy: scot\n",
      "❌ Failed: Function 'truncate_number' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_2_Tulu_3_cot.py ===\n",
      "→ Problem: humaneval_HumanEval_2, Model: Tulu, Strategy: 3\n",
      "❌ Failed: Function 'truncate_number' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_2_Tulu_3_scot.py ===\n",
      "→ Problem: humaneval_HumanEval_2, Model: Tulu, Strategy: 3\n",
      "❌ Failed: Function 'truncate_number' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_3_Llama_cot.py ===\n",
      "→ Problem: humaneval_HumanEval_3, Model: Llama, Strategy: cot\n",
      "❌ Failed: Function 'below_zero' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_3_Llama_scot.py ===\n",
      "→ Problem: humaneval_HumanEval_3, Model: Llama, Strategy: scot\n",
      "❌ Failed: Function 'below_zero' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_3_Tulu_3_cot.py ===\n",
      "→ Problem: humaneval_HumanEval_3, Model: Tulu, Strategy: 3\n",
      "❌ Failed: Function 'below_zero' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_3_Tulu_3_scot.py ===\n",
      "→ Problem: humaneval_HumanEval_3, Model: Tulu, Strategy: 3\n",
      "❌ Failed: Function 'below_zero' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_4_Llama_cot.py ===\n",
      "→ Problem: humaneval_HumanEval_4, Model: Llama, Strategy: cot\n",
      "❌ Failed: Function 'mean_absolute_deviation' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_4_Llama_scot.py ===\n",
      "→ Problem: humaneval_HumanEval_4, Model: Llama, Strategy: scot\n",
      "❌ Failed: Function 'mean_absolute_deviation' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_4_Tulu_3_cot.py ===\n",
      "→ Problem: humaneval_HumanEval_4, Model: Tulu, Strategy: 3\n",
      "❌ Failed: Function 'mean_absolute_deviation' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_4_Tulu_3_scot.py ===\n",
      "→ Problem: humaneval_HumanEval_4, Model: Tulu, Strategy: 3\n",
      "❌ Failed: Function 'mean_absolute_deviation' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_5_Llama_cot.py ===\n",
      "→ Problem: humaneval_HumanEval_5, Model: Llama, Strategy: cot\n",
      "❌ Failed: Function 'intersperse' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_5_Llama_scot.py ===\n",
      "→ Problem: humaneval_HumanEval_5, Model: Llama, Strategy: scot\n",
      "❌ Failed: Function 'intersperse' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_5_Tulu_3_cot.py ===\n",
      "→ Problem: humaneval_HumanEval_5, Model: Tulu, Strategy: 3\n",
      "❌ Failed: Function 'intersperse' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_5_Tulu_3_scot.py ===\n",
      "→ Problem: humaneval_HumanEval_5, Model: Tulu, Strategy: 3\n",
      "❌ Failed: Function 'intersperse' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_6_Llama_cot.py ===\n",
      "→ Problem: humaneval_HumanEval_6, Model: Llama, Strategy: cot\n",
      "❌ Failed: Function 'parse_nested_parens' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_6_Llama_scot.py ===\n",
      "→ Problem: humaneval_HumanEval_6, Model: Llama, Strategy: scot\n",
      "❌ Failed: Function 'parse_nested_parens' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_6_Tulu_3_cot.py ===\n",
      "→ Problem: humaneval_HumanEval_6, Model: Tulu, Strategy: 3\n",
      "❌ Failed: Function 'parse_nested_parens' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_6_Tulu_3_scot.py ===\n",
      "→ Problem: humaneval_HumanEval_6, Model: Tulu, Strategy: 3\n",
      "❌ Failed: Function 'parse_nested_parens' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_7_Llama_cot.py ===\n",
      "→ Problem: humaneval_HumanEval_7, Model: Llama, Strategy: cot\n",
      "❌ Failed: Function 'filter_by_substring' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_7_Llama_scot.py ===\n",
      "→ Problem: humaneval_HumanEval_7, Model: Llama, Strategy: scot\n",
      "❌ Failed: Function 'filter_by_substring' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_7_Tulu_3_cot.py ===\n",
      "→ Problem: humaneval_HumanEval_7, Model: Tulu, Strategy: 3\n",
      "❌ Failed: Function 'filter_by_substring' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_7_Tulu_3_scot.py ===\n",
      "→ Problem: humaneval_HumanEval_7, Model: Tulu, Strategy: 3\n",
      "❌ Failed: Function 'filter_by_substring' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_8_Llama_cot.py ===\n",
      "→ Problem: humaneval_HumanEval_8, Model: Llama, Strategy: cot\n",
      "❌ Failed: Function 'sum_product' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_8_Llama_scot.py ===\n",
      "→ Problem: humaneval_HumanEval_8, Model: Llama, Strategy: scot\n",
      "❌ Failed: Function 'sum_product' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_8_Tulu_3_cot.py ===\n",
      "→ Problem: humaneval_HumanEval_8, Model: Tulu, Strategy: 3\n",
      "❌ Failed: Function 'sum_product' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_8_Tulu_3_scot.py ===\n",
      "→ Problem: humaneval_HumanEval_8, Model: Tulu, Strategy: 3\n",
      "❌ Failed: Function 'sum_product' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_9_Llama_cot.py ===\n",
      "→ Problem: humaneval_HumanEval_9, Model: Llama, Strategy: cot\n",
      "❌ Failed: Function 'rolling_max' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_9_Llama_scot.py ===\n",
      "→ Problem: humaneval_HumanEval_9, Model: Llama, Strategy: scot\n",
      "❌ Failed: Function 'rolling_max' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_9_Tulu_3_cot.py ===\n",
      "→ Problem: humaneval_HumanEval_9, Model: Tulu, Strategy: 3\n",
      "❌ Failed: Function 'rolling_max' not found in the code\n",
      "\n",
      "=== Running humaneval_HumanEval_9_Tulu_3_scot.py ===\n",
      "→ Problem: humaneval_HumanEval_9, Model: Tulu, Strategy: 3\n",
      "❌ Failed: Function 'rolling_max' not found in the code\n",
      "\n",
      "======================================================================\n",
      "Results saved to results.json\n",
      "\n",
      "======================================================================\n",
      "SUMMARY: 3/60 solutions passed\n",
      "======================================================================\n",
      "Solutions:\n",
      "  ✅ Passed:    3 (  5.0%)\n",
      "  ❌ Failed:   57 ( 95.0%)\n",
      "======================================================================\n",
      "\n",
      "By Model:\n",
      "  Llama     :  2/30 solutions (  6.7%) |    0/   0 tests (  0.0%)\n",
      "  Tulu      :  1/30 solutions (  3.3%) |    0/   0 tests (  0.0%)\n",
      "\n",
      "By Strategy:\n",
      "  3         :  1/30 solutions (  3.3%) |    0/   0 tests (  0.0%)\n",
      "  cot       :  1/15 solutions (  6.7%) |    0/   0 tests (  0.0%)\n",
      "  scot      :  1/15 solutions (  6.7%) |    0/   0 tests (  0.0%)\n",
      "\n",
      "✅ Passed solutions:\n",
      "   - humaneval_HumanEval_0_Llama_cot.py (unknown/unknown tests)\n",
      "   - humaneval_HumanEval_0_Llama_scot.py (unknown/unknown tests)\n",
      "   - humaneval_HumanEval_0_Tulu_3_cot.py (unknown/unknown tests)\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import traceback\n",
    "import sys\n",
    "from io import StringIO\n",
    "\n",
    "GENERATED_DIR = \"generated\"\n",
    "PROBLEMS_FILE = \"problems.json\"\n",
    "\n",
    "\n",
    "def check_correctness(problem, code_file):\n",
    "    \"\"\"Execute the code file and run HumanEval+ tests, tracking individual test results.\"\"\"\n",
    "    try:\n",
    "        # Create a namespace for execution\n",
    "        namespace = {}\n",
    "        \n",
    "        # Load and execute user's code\n",
    "        with open(code_file, \"r\") as f:\n",
    "            code = f.read()\n",
    "        exec(code, namespace)\n",
    "        \n",
    "        # Check if function exists\n",
    "        entry_point = problem['entry_point']\n",
    "        if entry_point not in namespace:\n",
    "            return {\n",
    "                \"error\": f\"Function '{entry_point}' not found in the code\",\n",
    "                \"tests_passed\": 0,\n",
    "                \"tests_total\": 0\n",
    "            }\n",
    "        \n",
    "        candidate = namespace[entry_point]\n",
    "        \n",
    "        # Execute the test code to get the check function\n",
    "        exec(problem['test'], namespace)\n",
    "        \n",
    "        if 'check' not in namespace:\n",
    "            return {\n",
    "                \"error\": \"Test function 'check' not found\",\n",
    "                \"tests_passed\": 0,\n",
    "                \"tests_total\": 0\n",
    "            }\n",
    "        \n",
    "        # Now we need to count individual test cases\n",
    "        # We'll modify the check function to run tests individually\n",
    "        test_code = problem['test']\n",
    "        \n",
    "        # Parse the test code to extract inputs\n",
    "        # The test code has a structure with 'inputs = [...]'\n",
    "        exec_ns = {}\n",
    "        exec(test_code, exec_ns)\n",
    "        \n",
    "        # Try to get inputs from the check function's local scope\n",
    "        # We'll need to extract this more carefully\n",
    "        inputs = []\n",
    "        if 'inputs' in test_code:\n",
    "            # Execute just the inputs part\n",
    "            try:\n",
    "                for line in test_code.split('\\n'):\n",
    "                    if line.strip().startswith('inputs = '):\n",
    "                        exec(line, exec_ns)\n",
    "                        inputs = exec_ns.get('inputs', [])\n",
    "                        break\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        total_tests = len(inputs) if inputs else 0\n",
    "        \n",
    "        # If we can't extract inputs, just run the check function normally\n",
    "        if total_tests == 0:\n",
    "            try:\n",
    "                namespace['check'](candidate)\n",
    "                # If it passes, we don't know how many tests, so estimate\n",
    "                return {\n",
    "                    \"passed\": True,\n",
    "                    \"error\": None,\n",
    "                    \"tests_passed\": \"unknown\",\n",
    "                    \"tests_total\": \"unknown\"\n",
    "                }\n",
    "            except AssertionError as e:\n",
    "                return {\n",
    "                    \"passed\": False,\n",
    "                    \"error\": f\"Test failed: {str(e)}\",\n",
    "                    \"tests_passed\": \"unknown\",\n",
    "                    \"tests_total\": \"unknown\"\n",
    "                }\n",
    "            except Exception as e:\n",
    "                return {\n",
    "                    \"passed\": False,\n",
    "                    \"error\": f\"Runtime error: {str(e)}\",\n",
    "                    \"tests_passed\": 0,\n",
    "                    \"tests_total\": 0\n",
    "                }\n",
    "        \n",
    "        # Run each test individually\n",
    "        passed_tests = 0\n",
    "        failed_tests = []\n",
    "        \n",
    "        for i, test_input in enumerate(inputs):\n",
    "            try:\n",
    "                result = candidate(*test_input)\n",
    "                # We can't verify correctness without expected output\n",
    "                # So we just check if it runs without error\n",
    "                passed_tests += 1\n",
    "            except Exception as e:\n",
    "                failed_tests.append((i, test_input, str(e)))\n",
    "        \n",
    "        # Also run the full check to see if assertions pass\n",
    "        try:\n",
    "            namespace['check'](candidate)\n",
    "            all_passed = True\n",
    "        except AssertionError as e:\n",
    "            all_passed = False\n",
    "            error_msg = str(e)\n",
    "        except Exception as e:\n",
    "            all_passed = False\n",
    "            error_msg = str(e)\n",
    "        \n",
    "        if all_passed:\n",
    "            return {\n",
    "                \"passed\": True,\n",
    "                \"error\": None,\n",
    "                \"tests_passed\": total_tests,\n",
    "                \"tests_total\": total_tests\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"passed\": False,\n",
    "                \"error\": error_msg if 'error_msg' in locals() else \"Some assertions failed\",\n",
    "                \"tests_passed\": passed_tests,\n",
    "                \"tests_total\": total_tests,\n",
    "                \"failed_test_samples\": failed_tests[:3]\n",
    "            }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"error\": f\"Execution failed: {str(e)}\\n{traceback.format_exc()}\",\n",
    "            \"tests_passed\": 0,\n",
    "            \"tests_total\": 0\n",
    "        }\n",
    "\n",
    "\n",
    "def main():\n",
    "    with open(PROBLEMS_FILE) as f:\n",
    "        problems = json.load(f)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for file in sorted(os.listdir(GENERATED_DIR)):\n",
    "        if not file.endswith(\".py\"):\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(GENERATED_DIR, file)\n",
    "        parts = file[:-3].split(\"_\")\n",
    "        if len(parts) < 4:\n",
    "            print(f\"Skipping {file} (invalid name format)\")\n",
    "            continue\n",
    "\n",
    "        # Parse: humaneval_HumanEval_X_model_strategy\n",
    "        problem_id = \"_\".join(parts[:3])  # humaneval_HumanEval_X\n",
    "        model = parts[3]\n",
    "        strategy = parts[4] if len(parts) > 4 else \"unknown\"\n",
    "\n",
    "        problem = next((p for p in problems if p[\"id\"] == problem_id), None)\n",
    "        if not problem:\n",
    "            print(f\"⚠️ Problem {problem_id} not found in problems.json\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n=== Running {file} ===\")\n",
    "        print(f\"→ Problem: {problem_id}, Model: {model}, Strategy: {strategy}\")\n",
    "\n",
    "        result = check_correctness(problem, file_path)\n",
    "\n",
    "        tests_passed = result.get('tests_passed', 0)\n",
    "        tests_total = result.get('tests_total', 0)\n",
    "        \n",
    "        if \"error\" in result and result[\"error\"]:\n",
    "            print(f\"❌ Failed: {result['error']}\")\n",
    "            if tests_total:\n",
    "                print(f\"   Tests: {tests_passed}/{tests_total}\")\n",
    "        else:\n",
    "            if result.get(\"passed\"):\n",
    "                print(f\"✅ All tests passed! ({tests_passed}/{tests_total})\")\n",
    "            else:\n",
    "                print(f\"⚠️ Partial pass: {tests_passed}/{tests_total} tests\")\n",
    "\n",
    "        results.append({\n",
    "            \"file\": file,\n",
    "            \"problem_id\": problem_id,\n",
    "            \"model\": model,\n",
    "            \"strategy\": strategy,\n",
    "            \"passed\": result.get(\"passed\", False),\n",
    "            \"error\": result.get(\"error\"),\n",
    "            \"tests_passed\": tests_passed,\n",
    "            \"tests_total\": tests_total\n",
    "        })\n",
    "\n",
    "    # Save summary\n",
    "    with open(\"results.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Results saved to results.json\")\n",
    "    \n",
    "    # Print summary statistics\n",
    "    total_solutions = len(results)\n",
    "    passed_solutions = sum(1 for r in results if r.get('passed', False))\n",
    "    failed_solutions = total_solutions - passed_solutions\n",
    "    \n",
    "    # Calculate test statistics\n",
    "    total_tests_sum = sum(r.get('tests_total', 0) for r in results if isinstance(r.get('tests_total'), int))\n",
    "    passed_tests_sum = sum(r.get('tests_passed', 0) for r in results if isinstance(r.get('tests_passed'), int))\n",
    "    \n",
    "    if total_solutions > 0:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"SUMMARY: {passed_solutions}/{total_solutions} solutions passed\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Solutions:\")\n",
    "        print(f\"  ✅ Passed:  {passed_solutions:3d} ({100*passed_solutions/total_solutions:5.1f}%)\")\n",
    "        print(f\"  ❌ Failed:  {failed_solutions:3d} ({100*failed_solutions/total_solutions:5.1f}%)\")\n",
    "        \n",
    "        if total_tests_sum > 0:\n",
    "            print(f\"\\nIndividual Tests:\")\n",
    "            print(f\"  ✅ Passed:  {passed_tests_sum:4d}/{total_tests_sum:4d} ({100*passed_tests_sum/total_tests_sum:5.1f}%)\")\n",
    "            print(f\"  ❌ Failed:  {total_tests_sum - passed_tests_sum:4d}/{total_tests_sum:4d} ({100*(total_tests_sum - passed_tests_sum)/total_tests_sum:5.1f}%)\")\n",
    "        \n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # Breakdown by model and strategy\n",
    "        from collections import defaultdict\n",
    "        by_model = defaultdict(lambda: {'passed': 0, 'total': 0, 'tests_passed': 0, 'tests_total': 0})\n",
    "        by_strategy = defaultdict(lambda: {'passed': 0, 'total': 0, 'tests_passed': 0, 'tests_total': 0})\n",
    "        \n",
    "        for r in results:\n",
    "            model = r.get('model', 'unknown')\n",
    "            strategy = r.get('strategy', 'unknown')\n",
    "            passed = r.get('passed', False)\n",
    "            tests_p = r.get('tests_passed', 0) if isinstance(r.get('tests_passed'), int) else 0\n",
    "            tests_t = r.get('tests_total', 0) if isinstance(r.get('tests_total'), int) else 0\n",
    "            \n",
    "            by_model[model]['total'] += 1\n",
    "            by_strategy[strategy]['total'] += 1\n",
    "            by_model[model]['tests_passed'] += tests_p\n",
    "            by_model[model]['tests_total'] += tests_t\n",
    "            by_strategy[strategy]['tests_passed'] += tests_p\n",
    "            by_strategy[strategy]['tests_total'] += tests_t\n",
    "            \n",
    "            if passed:\n",
    "                by_model[model]['passed'] += 1\n",
    "                by_strategy[strategy]['passed'] += 1\n",
    "        \n",
    "        if len(by_model) > 1:\n",
    "            print(\"\\nBy Model:\")\n",
    "            for model, stats in sorted(by_model.items()):\n",
    "                sol_pct = 100 * stats['passed'] / stats['total'] if stats['total'] > 0 else 0\n",
    "                test_pct = 100 * stats['tests_passed'] / stats['tests_total'] if stats['tests_total'] > 0 else 0\n",
    "                print(f\"  {model:10s}: {stats['passed']:2d}/{stats['total']:2d} solutions ({sol_pct:5.1f}%) | {stats['tests_passed']:4d}/{stats['tests_total']:4d} tests ({test_pct:5.1f}%)\")\n",
    "        \n",
    "        if len(by_strategy) > 1:\n",
    "            print(\"\\nBy Strategy:\")\n",
    "            for strategy, stats in sorted(by_strategy.items()):\n",
    "                sol_pct = 100 * stats['passed'] / stats['total'] if stats['total'] > 0 else 0\n",
    "                test_pct = 100 * stats['tests_passed'] / stats['tests_total'] if stats['tests_total'] > 0 else 0\n",
    "                print(f\"  {strategy:10s}: {stats['passed']:2d}/{stats['total']:2d} solutions ({sol_pct:5.1f}%) | {stats['tests_passed']:4d}/{stats['tests_total']:4d} tests ({test_pct:5.1f}%)\")\n",
    "        \n",
    "        # Show which solutions passed\n",
    "        if passed_solutions > 0:\n",
    "            print(f\"\\n✅ Passed solutions:\")\n",
    "            for r in results:\n",
    "                if r.get('passed', False):\n",
    "                    tp = r.get('tests_passed', '?')\n",
    "                    tt = r.get('tests_total', '?')\n",
    "                    print(f\"   - {r['file']} ({tp}/{tt} tests)\")\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
